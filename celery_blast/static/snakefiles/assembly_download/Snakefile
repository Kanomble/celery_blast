import pandas as pd


#SETUP COMMUNICATION WITH API
'''
import os
import sys
import django
sys.path.append("/blast/reciprocal_blast")
os.environ["DJANGO_SETTINGS_MODULE"] = "celery_blast.settings"
django.setup()
from blast_project.models import BlastDatabase
from celery.result import AsyncResult
from django_celery_results.models import TaskResult
from time import sleep
def update_taskresult_model():
    task = TaskResult.objects.get(task_id=config['task_id'])
    update task results (json) dict ..
    return
'''

configfile: "./snakefile_config"

#SETUP UTILITY FUNCTIONS
def get_ftp_paths_and_taxids_from_summary_file(filename):
    dataframe = pd.read_table(filename,header=0,index_col=0,delimiter=",")
    #there shouldnt be any duplicates
    dataframe = dataframe[dataframe['ftp_path'].duplicated() == False]
    return dataframe['ftp_path'],dataframe['species_taxid']

def get_progress():
    fd = open('snakemake_progress.log','r')
    progress = round(float(fd.readlines()[-1]),3)
    fd.close()
    return progress


def write_progress():
    try:
        progress_steps = round(100/(len(FTP_PATHS)*2),3)
        progress_log = open('snakemake_progress.log','a')
        progress = get_progress()
        progress += progress_steps
        progress_log.write(str(progress)+'\n')
        progress_log.close()
        return progress
    except Exception as e:
        raise Exception("error writing logfile excpetion : {}".format(e))

transform_ftp_path = lambda file: file.split('/')[-1].rstrip(file[-3:])

#SETUP INPUT AND OUTPUT FILE VARIABLES
FTP_PATHS, TAXIDS = get_ftp_paths_and_taxids_from_summary_file(config['db_summary'])
#TODO create with pandas map function for large files!
FASTA_FILES = [transform_ftp_path(ftp_path) for ftp_path in FTP_PATHS]
DATABASES = [download+'.pdb' for download in FASTA_FILES]

onstart:
    progress_log = open('snakemake_progress.log','w')
    progress = 0
    progress_log.write(str(progress)+'\n')
    progress_log.close()

onerror:
    progress_log = open('snakemake_progress.log','w')
    progress = "ERROR"
    progress_log.write(str(progress)+'\n')
    progress_log.close()

rule all:
    input: FASTA_FILES, DATABASES#expand(["{database}"],database=[file+".pdb" for file in FASTA_FILES]) #
    run:
        #append django working directory to child process path
        #TITLE combined_db
        #DBLIST "prot_1_db.faa" "prot_2_db.faa"
        #update_taskresult_model()
        #sleep(20)
        alias_file_name = config['db_summary']+'.database.pal'
        alias_file=open(alias_file_name,'w')
        alias_file.write("TITLE {}\n".format(config['db_summary']+'.database'))
        alias_file.write("DBLIST")
        for database in FASTA_FILES:
            alias_file.write(" \""+database+"\"")
        alias_file.write("\n")
        alias_file.close()
        progress_log = open('snakemake_progress.log','a')
        progress_log.write(str(100)+'\n')
        progress_log.close()

rule download_and_decompress:
    input: config['db_summary']
    output: FASTA_FILES
    run:
        error_log=open('download_error.log','w')
        for file in FTP_PATHS:
            for attempt in range(10):
                try:
                    gunzip_output = transform_ftp_path(file)
                    process = shell("wget -qO- {file} | gzip -d > {gunzip_output}")
                    progress = write_progress()
                except Exception as e:
                    print("exception : {}".format(e))
                    error_log.write("{} {}\n".format(file,attempt))
                else:
                    break
            else:
                error_log.close()
                raise Exception
        error_log.close()

rule format_fasta_files_to_blast_databases:
    input: FASTA_FILES #expand(["{fastafile}"],fastafile=FASTA_FILES) #FASTA_FILES#
    output: DATABASES #expand(["{database}"],database=[file+".pdb" for file in FASTA_FILES]) #DATABASES #
    run:
        error_log=open('database_formatting_error.log','w')
        for fastafile,taxid in zip(FASTA_FILES,TAXIDS):
            for attempt in range(3):
                try:
                    #devnull rly necessary?
                    process = shell("makeblastdb -in {fastafile} -dbtype prot -out {fastafile} -taxid {taxid} -parse_seqids > /dev/null")
                    progress = write_progress()
                    #print("fastafile: {}\ttaxid: {}".format(fastafile,taxid))
                except:
                    error_log.write("{} {}\n".format(fastafile,attempt))
                else:
                    break
            else:
                error_log.close()
                raise Exception
        error_log.close()



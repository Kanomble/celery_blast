#snakemake configuration is defined in the snakefile_config file,
#which is created by a BlastProject model function
configfile: "./snakefile_config"
import pandas as pd
from math import ceil
from Bio import Entrez

#PIPELINE PREPARATION
#filling the QSEQIDS list for the input wildcard "qseqid" in the all rule
QSEQIDS = []
with open(config['query_sequence'],'r') as qseqfile:
	for line in qseqfile.readlines():
		if ">" in line:
			qseq_id = line.split(" ")[0].split(">")[1].split(".")[0]
			QSEQIDS.append(qseq_id)

# collection rule
rule all:
	input:
		"interactive_bokeh_plot.html",
		"blastp_fw_out.table",
		"bw_queries_gi_list_for_blastdbcmd.txt",
		"reciprocal_results_with_taxonomy.csv",
		expand("{qseqid}/results_rbhs.html", qseqid=QSEQIDS),
		"query_sequence_information.html",
		"blastp_fw_out_reformatted.table",
		expand("{qseqid}/results_rbhs.html",qseqid=QSEQIDS),
		"query_domains.tsf", "query_domains.html",
		expand("{qseqid}/msa_phylo_rbh_table.tsf",qseqid=QSEQIDS),
		expand("{qseqid}/msa_phylo_sequences.faa",qseqid=QSEQIDS),
		expand("{qseqid}/msa_phylo_sequences.msa", qseqid=QSEQIDS),
		expand("{qseqid}/msa_phylo_sequences_trimmed.msa", qseqid=QSEQIDS),
		expand("{qseqid}/msa_phylo_sequences_trimmed.html", qseqid=QSEQIDS),
		expand("{qseqid}/msa_phylo_sequences.tree", qseqid=QSEQIDS),
		expand("{qseqid}/msa_phylo_sequences.html", qseqid=QSEQIDS),
		expand("{qseqid}/genome_assembly_table.csv", qseqid=QSEQIDS),
		"plot_amount_hits_of_target_taxon.png",
		"plot_evalue_distribution.png",
		"query_domains_rpsbproc.out",
		"rpsbproc_query_domains.html",
		"rpsbproc_query_sites.html",
		"rpsbproc_query_domains.csv",
		"rpsbproc_query_sites.csv"


#RULE 1
#Forward BLAST - inference of target sequences within the search space defined by the entrez_query and the chosen database
rule forward_blast:
	input: queries=config['query_sequence']
	output: results="blastp_fw_out.table"
	params: word_size=config['fw_word_size'], e_value=config['fw_e_value'], num_alignments=config['fw_num_alignments'], num_threads=config['fw_num_threads'], database=config['blastdb'], entrez_query=config['entrez_query']
	log: log="log/forward_blast.log", log_blastp="log/log_blastp_forward_blast.log"
	run:
		with open(log[0], "w") as logfile:
			try:
				logfile.write("INFO:starting forward blast search against: {}\n".format(params.database))
				logfile.write("INFO:reading query file ...\n")
				query_dict = {}
				with open(input[0],"r") as qfile:
					for line in qfile.readlines():
						if line.startswith(">"):
							header = line
							if line not in list(query_dict.keys()):
								query_dict[line] = ""
						else:
							if header:
								query_dict[header] += line
				logfile.write("INFO:writing temporary query files and conducting blast searches:\n")
				logfile.write("INFO:temporary query files are named temp_fw_seq_{index}.faa.\n"
							  "\tThe index is the index number of the header of the query sequence within the provided query file.\n")
				logfile.write("INFO:BLAST searches are conducted on each single file to prevent errors raised by the NCBI servers...\n")
				logfile.write("INFO:this likely occurs if the search is not restricted to any particular taxon.\n")
				logfile.write("INFO:entrez_query:{}\n".format(params.entrez_query))
				for index, header in enumerate(query_dict.keys()):
					logfile.write("\tINFO:working with query file: {}\n".format(header.split(" ")[0].split(">")[1]))
					with open("temp_fw_seq_{}.faa".format(index),"w") as tempfile:
						tempfile.write(header)
						tempfile.write(query_dict[header])

				inter_blast_out = open("blast.table",'w')
				for index, header in enumerate(query_dict.keys()):
					if (params.entrez_query.strip() != "" and params.entrez_query.strip() != "all[organism]"):
						entrez_query = params.entrez_query.strip()
						cmd_string = "blastp" + " -db " + params.database + \
									 " -outfmt \"6 qseqid sseqid pident evalue bitscore qgi sgi sacc staxids sscinames scomnames stitle slen\"" \
									 " -out " + "current_search_fw_blast.table" + \
									 " -word_size " + str(params.word_size) + \
									 " -evalue " + str(params.e_value) + \
									 " -num_alignments " + str(params.num_alignments) + \
									 " -query " + "temp_fw_seq_{}.faa".format(index) + \
									 " -remote" + \
									 " -entrez_query \"" + entrez_query + "\"" + " 2> {log.log_blastp}"
					elif (params.entrez_query.strip() == "all[organism]" or params.entrez_query.strip() == ""):
						cmd_string = "blastp" + " -db " + params.database + \
									 " -outfmt \"6 qseqid sseqid pident evalue bitscore qgi sgi sacc staxids sscinames scomnames stitle slen\"" \
									 " -out " + "current_search_fw_blast.table" + \
									 " -word_size " + str(params.word_size) + \
									 " -evalue " + str(params.e_value) + \
									 " -num_alignments " + str(params.num_alignments) + \
									 " -query " + "temp_fw_seq_{}.faa".format(index) + \
									 " -remote" + " 2> {log.log_blastp}"
					logfile.write("INFO: cmd string for the forward blast:\n")
					logfile.write("\tINFO: {}\n".format(cmd_string))
					# BLAST search!
					shell(cmd_string)
					with open("current_search_fw_blast.table",'r') as interfilename:
						for line in interfilename.readlines():
							inter_blast_out.write(line)
					logfile.write("\tINFO:DONE\n")
					logfile.write("\tINFO:deleting intermediate files ...\n")
					shell("rm current_search_fw_blast.table")
					shell("rm temp_fw_seq_{}.faa".format(index))

				inter_blast_out.close()
				logfile.write("INFO:renaming blast.table to snakemake output: blastp_fw_out.table\n")
				#changing output filename to snakemake output name
				shell("mv blast.table {output.results}")
				logfile.write("DONE\n")
			except Exception as e:
				logfile.write("ERROR: Exception occurred during remote blast search: {}\n".format(e))
				raise Exception("[-] ERROR during execution of remote forward BLAST with following exception: {}".format(e))

#RULE 2
#collects the subject id's of the forward BLAST and writes those id's into a file that is used by the bw_query_preparation rule (RULE 3)
rule fw_result_processing:
	input: fw_res="blastp_fw_out.table", fw_blast_logs="log/forward_blast.log", fw_queries=config["query_sequence"]
	params: taxid=config["bw_taxid"], user_email=config["user_email"], forward_dataframe="blastp_fw_out.table"
	output: gi_list="bw_queries_gi_list_for_blastdbcmd.txt"
	log: log="log/fw_result_processing.log"
	run:
		with open(log[0],'w') as logfile:
			try:
				logfile.write("INFO:working with forward blast results: writing target sequences into output file\n")
				try:
					fw_results = pd.read_table(input[0], header=None)
				except pd.errors.EmptyDataError as e:
					fw_results = pd.DataFrame(columns=range(0, 13))
					logfile.write("WARNING:there are no hits in the forward BLAST output, creating empty dataframe ...\n")



				logfile.write(('INFO:done writing {} target sequences into output file\n'.format(len(fw_results[7][:].unique()))))
				logfile.write("INFO:checking if query sequences are present in forward dataframe ...\n")
				logfile.write("INFO:open query file ...\n")
				queries = {}
				with open(input.fw_queries,"r") as queryfile:
					for line in queryfile.readlines():
						if ">" in line:
							prot_id = line.rstrip().split(">")[1].split(' ')[0]
							queries[prot_id] = ""
						else:
							queries[prot_id] += line.rstrip()

				try:
					Entrez.email = params.user_email
					search = Entrez.efetch(id=params.taxid,db='taxonomy',retmode='xml')
					record = Entrez.read(search)
					search.close()
					scientific_name = record[0]['ScientificName']
				except:
					logfile.write("INFO:haven't found scientific name for provided taxonomic node: {}\n".format(params.taxid))
					logfile.write("INFO:scientific name: no information ...\n")
					scientific_name = "no information"

				for query in queries.keys():
					temp_data = fw_results.copy()
					temp_data[7] = temp_data[7].apply(lambda x: x.split(".")[0])
					temp_data = temp_data[temp_data[7] == query.split(".")[0]]
					if len(temp_data) == 0:
						logfile.write("INFO:working with query: {}\n".format(query))
						logfile.write("\tINFO:adding placeholder for query sequence: {} to forward BLAST dataframe\n".format(query))
						new_forward_entry = {0: query, 1: query, 2: 100.0, 3: 0.0, 4: 600, 5: 0, 6: 0, 7: query,
											 8: params.taxid,
											 9: scientific_name,
											 10: scientific_name,
											 11: "artificially added RBH - not in FW database",
											 12: len(queries[query])}
						fw_results = fw_results.append(new_forward_entry,ignore_index=True)

				#writing gi file
				with open(output[0],"w+") as out:
					# remove duplicates
					for gi in fw_results[7][:].unique():
						out.write(gi+"\n")

				logfile.write("INFO:writing new forward dataframe to disc ...\n")
				fw_results.to_csv(params.forward_dataframe, index=False, header=False, sep="\t")

				logfile.write("DONE\n")
			except Exception as e:
				logfile.write("ERROR:{}\n".format(e))
				raise Exception("ERROR in forward blast result processing: {}".format(e))

#RULE 3
#prepares the backward query fasta file for the backward blast
rule bw_query_preparation:
	input: gi_list="bw_queries_gi_list_for_blastdbcmd.txt", fw_result_processing_logs="log/fw_result_processing.log", fw_queries=config["query_sequence"]
	output: fasta_file="bw_queries.faa"
	params: user_email=config['user_email'], database=config['blastdb']
	log: log="log/bw_query_preparation.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/remote_searches/extract_subject_sequences_from_NCBI_edirect.py"

#RULE 4
#backward blast rule. the backward blast is divided into chunks of 100 query sequences.
rule backward_blast:
	input: bw_queries="bw_queries.faa", bw_query_preparation_logs="log/bw_query_preparation.log"
	output: "blastp_bw_out.table"
	params: word_size=config['bw_word_size'],e_value=config['bw_e_value'],num_alignments=config['bw_num_alignments'],max_hsps=config['bw_max_hsps'],num_threads=config['bw_num_threads'],taxid=config['bw_taxid'],database=config['backwarddb'],intermediate_filename="inter_blast.table"
	log: log="log/backward_blast.log"
	run:
		with open(log[0],'w') as logfile:
			try:
				logfile.write("INFO:starting backward blast ...\n")
				step = 100
				sequence_dict = {}
				#loading query sequences into python dictionary
				with open(input[0],'r') as infile:
					for line in infile.readlines():
						if line.startswith(">"):
							header = line.split(" ")[0].split(">")[-1]
							sequence_dict[header] = ''
						if header:
							sequence_dict[header] += line

				#preparation of transient input files for the backward blast. maximal 100 query sequences will reside in the input fasta files
				filenames = ["bw_queries_" + str(i) + ".faa" for i in range(ceil(len(sequence_dict.keys()) / float(step)))]
				logfile.write("INFO:working with: {} input fasta files\n".format(len(filenames)))
				keys = list(sequence_dict.keys())
				start = 0
				filecounter = 0
				end = step
				while start < len(keys) and filecounter <= len(filenames):
					with open(filenames[filecounter],'w') as outfile:
						for key in keys[start:end]:
							for line in sequence_dict[key]:
								outfile.write(line)
						start = end
						end += step
						filecounter += 1

				#backward blast procedure
				inter_blast_out = open("blast.table",'w')
				for index, file in enumerate(filenames):
					logfile.write("INFO:working with input sequence file {}\n".format(index))
					shell("blastp -db {params.database} -outfmt \"6 qseqid sseqid pident evalue bitscore qgi sgi sacc staxids sscinames scomnames stitle slen\" -out {params.intermediate_filename} -taxids {params.taxid} -word_size {params.word_size} -evalue {params.e_value} -num_alignments {params.num_alignments} -max_hsps {params.max_hsps} -num_threads {params.num_threads} -query {file} 2> {log}")
					with open(params.intermediate_filename,'r') as interfilename:
						for line in interfilename.readlines():
							inter_blast_out.write(line)
				inter_blast_out.close()
				logfile.write("INFO:done with blastp on all input files, removing intermediate query files ...\n")
				#removing intermediate query files
				for file in filenames:
					shell("rm {file}")
				#removing intermediate blast table (table of the last backward blast run)
				shell("rm inter_blast.table")
				logfile.write("INFO:renaming blast.table to snakemake output: blastp_bw_out.table\n")
				#changing output filename to snakemake output name
				shell("mv blast.table {output}")
				logfile.write("DONE\n")
			except Exception as e:
				logfile.write("ERROR:{}\n".format(e))
				raise Exception("[-] ERROR during backward blast with exception: {}".format(e))

#RULE 5
#prepares the backward query fasta file for the backward blast
rule reformat_blast_forward_dataframe:
	input: fw_res="blastp_fw_out.table"
	output: new_fw_res="blastp_fw_out_reformatted.table", all_taxids_fw_res="blastp_fw_all_taxids_reformatted.table"
	log: log="log/reformat_blast_forward_dataframe.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/remote_searches/reformat_forward_blast_table.py"

#RULE 6
#extracting reciprocal best hits with pandas blast tables
rule reciprocal_best_hits:
	input: bw_res="blastp_bw_out.table", fw_res="blastp_fw_out_reformatted.table", reformatting_log="log/reformat_blast_forward_dataframe.log"
	output: rec_best_hits="reciprocal_best_hits_protein_ids.txt"
	params: bitscore_filter=config["bitscore_filter"]
	log: log="log/reciprocal_best_hits.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/extract_reciprocal_best_hits.py"

#RULE 7
#transforms the forward blast and backward blast tables with the rbhs table to a result output table with additional taxonomic information.
rule blast_tables_to_csv:
	input: fw_res="blastp_fw_out_reformatted.table", rec_res="reciprocal_best_hits_protein_ids.txt", query_file=config['query_sequence'], reciprocal_best_hits_log="log/reciprocal_best_hits.log"
	output: result_csv="reciprocal_results.csv", taxonomy_result_csv='reciprocal_results_with_taxonomy.csv'
	log: log="log/blast_tables_to_csv.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/blast_tables_to_orthologous_table.py"

#RULE 8
#transforming csv files to html files for the frontend
rule blast_tables_to_html:
	input: rec_res="reciprocal_results_with_taxonomy.csv", blast_tables_to_csv_log="log/blast_tables_to_csv.log"
	output: rec_html="reciprocal_results.html"
	log: log="log/blast_tables_to_html.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/blast_tables_to_html.py"

#RULE 9
#interactive plot creation with bokeh
rule create_interactive_plot:
	input: blast_results="reciprocal_results_with_taxonomy.csv", blast_tables_to_html_log="log/blast_tables_to_html.log"
	params: user_email=config['user_email']
	output: bokeh_plot="interactive_bokeh_plot.html"
	log:log="log/create_interactive_plot.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/remote_searches/bokeh_plot_for_remote_blast_project.py"


#RULE 10
#outputs a summary for the RBH inference and constructs directories for each query sequence
rule build_folders_with_hit_info_for_each_qseqid:
	input: result_csv="reciprocal_results_with_taxonomy.csv", query_file=config['query_sequence'], bokeh_plot_log="log/create_interactive_plot.log"
	output: hit_information="reciprocal_results_info.txt",queries=expand("{qseqid}/target_sequence_ids.txt",qseqid=QSEQIDS),
		rec_res_html=expand("{qseqid}/results_rbhs.html",qseqid=QSEQIDS),rec_res_table=expand("{qseqid}/rbh_table.tsf",qseqid=QSEQIDS),
		result_statistics=expand("{qseqid}/basic_statistics.png",qseqid=QSEQIDS)
	params:	project_id=config['project_id']
	log: log="log/build_folders_with_hit_info_for_each_qseqid.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/build_folders_with_hit_info_for_each_qseqid.py"

#RULE 11
#creates a html table with information of the query sequences
rule query_sequences_to_html_table:
	input: target_file=config['query_sequence'], hit_info_log="log/build_folders_with_hit_info_for_each_qseqid.log"
	params: email=config['user_email']
	output: output_html="query_sequence_information.html", output_csv="query_sequence_information.csv"
	log: log="log/query_sequences_to_html_table.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/query_sequences_to_html_table.py"

#RULE 12
#extracts sequences of RBHs from the forward database
rule extract_rbh_sequences:
	input: target_ids="{qseqid}/target_sequence_ids.txt", target_fasta="bw_queries.faa", query_sequence_table_log="log/query_sequences_to_html_table.log"
	output: "{qseqid}/target_sequences_raw.faa"
	log: log="log/{qseqid}/extract_subject_sequences_from_database.log"
	run:
		with open(log.log, "w") as logfile:
			logfile.write("INFO:creating target sequence fasta file for query sequences\n")
			try:
				logfile.write("INFO:reading target id file ...\n")
				target_ids = []
				with open(input.target_ids, "r") as idfile:
					for line in idfile.readlines():
						target_ids.append(line.rstrip())
				logfile.write("INFO:reading input fasta file and creating output fasta file ...\n")
				with open(output[0],"w") as outputfasta:
					with open(input.target_fasta, "r") as inputfasta:
						switch = 0
						for line in inputfasta.readlines():
							if line.startswith(">"):
								header = line.split(" ")
								header = header[0].split(">")[-1]
								if header in target_ids:
									switch = 1
									outputfasta.write(line)
								else:
									switch = 0
							elif switch == 1:
								outputfasta.write(line)

				logfile.write("DONE\n")
			except Exception as e:
				logfile.write("ERROR:error during creation of query sequence target sequences with exception: {}\n".format(e))
				raise Exception("[-] ERROR during creation of query sequence target sequence file with exception: {}".format(e))

#RULE 13
#checks if the query sequence resides in the target_sequences.faa file
rule check_if_query_sequence_is_in_target_sequences_faa:
	input: target_file=expand("{qseqid}/target_sequences_raw.faa",qseqid=QSEQIDS), query_file=config['query_sequence']
	output: expand("{qseqid}/target_sequences.faa",qseqid=QSEQIDS)
	params: qseqs=QSEQIDS
	log: log="log/check_if_query_sequence_is_in_target_sequences_faa.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/check_if_query_sequence_is_in_target_sequences_faa.py"

#RULE 14
#extracting target sequences for phylogenetic inference
rule extract_target_sequences_for_phylogeny:
	input: target_fasta="{qseqid}/target_sequences.faa",rec_res_table="{qseqid}/rbh_table.tsf", check_if_query_is_in_target_sequence_file_log="log/check_if_query_sequence_is_in_target_sequences_faa.log"
	output: msa_phylo_sequences="{qseqid}/msa_phylo_sequences.faa", msa_phylo_blast_table="{qseqid}/msa_phylo_rbh_table.tsf"
	params: max_sequences=config['max_rbhs_for_phylo']
	log: log="log/{qseqid}/extract_target_sequences_for_phylogeny.log"
	run:
		with open(log['log'],'a') as lfile:
			lfile.write("INFO:reading forward BLAST table and filter table based on max_sequences parameter: {}\n".format(
				params.max_sequences
			))
			# load RBH result dataframe and slice the bitscore sorted dataframe based on the max max_sequences param
			fw_df = pd.read_table(input.rec_res_table)
			# omit sequences not present within the multiple sequence alignment
			fw_df = fw_df[fw_df.query_info != "artificially added RBH - not in FW database"]

			if len(fw_df) > int(params.max_sequences):
				lfile.write("INFO:sorting BLAST table by bitscore with {} entries ...\n".format(len(fw_df)))
				fw_df = fw_df.sort_values(by="bitscore",ascending=False)
				lfile.write("INFO:applying threshold for the maximal number of sequences for MSA and Phylogeny ...\n")
				fw_df = fw_df[0:int(params.max_sequences)]
				lfile.write("INFO:after filtering {} sequences remain...\n".format(len(fw_df)))

			lfile.write("INFO:writing blast table for aligning with leafs in the phylogeny...\n")
			fw_df.to_csv(output.msa_phylo_blast_table,sep="\t", index=False)
			target_sequences = fw_df.sacc.to_list()

			#check if target sequence file is empty (or just contains the query sequence)
			#if this is the case, there is no need for producing a msa
			with open(input.target_fasta,'r') as infile:
				lines = infile.readlines()
				with open(output.msa_phylo_sequences,"w") as target_file:
					for line in lines:
						if line.startswith(">"):
							acc = line.split(" ")[0].split(">")[1]
							if acc in target_sequences:
								switch = True
							else:
								switch = False
						if switch == True:
							target_file.write(line)

#RULE 15
#multiple sequence alignment with mafft
#this rule shouldn't throw exceptions, if an error occurs an empty file is created
rule conduct_multiple_sequence_alignment_mafft:
	input: msa_phylo_sequences="{qseqid}/msa_phylo_sequences.faa", extract_sequences_log="log/{qseqid}/extract_target_sequences_for_phylogeny.log"
	output: target_sequences="{qseqid}/msa_phylo_sequences.msa"
	log: log="log/{qseqid}/msa.log"
	run:
		with open(log['log'],'a') as lfile:
			execute_mafft=True
			lfile.write("INFO:starting multiple sequence alignment procedure with mafft...\n")
			#check if target sequence file is empty (or just contains the query sequence)
			#if this is the case, there is no need for producing a msa
			with open(input[0],'r') as infile:
				if len(infile.readlines()) <= 1:
					lfile.write("WARNING:there are not enough sequences in the FASTA file for mafft...\n")
					execute_mafft=False
			try:
				if execute_mafft==True:
					#mafft execution
					mafft=shell("mafft --auto {input.msa_phylo_sequences} > {output} 2> {log}")
					lfile.write("INFO:ending mafft multiple sequence alignment\n")
					lfile.write("DONE")
				else:
					lfile.write("WARNING: producing empty alignment file with touch\n")
					#produce empty file for snakemake
					shell("touch {output}")
					lfile.write("DONE\n")
			except Exception as e:
				with open(log['log'],'w') as lfile:
					lfile.write("ERROR: in mafft with exception: {}\n".format(e))
					shell("touch {output}")

#RULE 16
#trimming of the alignment using trimal
#if there are no sequences in the msa an empty file is produced via touch
rule trimal:
	input: target_msa="{qseqid}/msa_phylo_sequences.msa", multiple_sequence_alignment_log="log/{qseqid}/msa.log"
	output: trimmed_sequences="{qseqid}/msa_phylo_sequences_trimmed.msa"
	log: trimal_log="log/{qseqid}/trimal.log"
	run:
		with open(log['trimal_log'],'a') as lfile:
			execute_trimal=True
			lfile.write("INFO:starting multiple sequence alignment trimming procedure with trimal...\n")
			#check if target sequence file is empty (or just contains the query sequence)
			#if this is the case, there is no need for producing a msa
			with open(input[0],'r') as infile:
				if len(infile.readlines()) <= 1:
					lfile.write("WARNING:there are not enough sequences in the msa file ...\n")
					execute_trimal=False
			try:
				if execute_trimal==True:
					#trimal execution
					trimal=shell("trimal -in {input.target_msa} -out {output.trimmed_sequences} -gt 0.8 -st 0.001 -cons 60 2> {log.trimal_log}")
					lfile.write("INFO:ending trimal multiple sequence alignment trimming\n")
					lfile.write("DONE")
				else:
					lfile.write("WARNING: producing empty trimmed alignment file with touch\n")
					#produce empty file for snakemake
					shell("touch {output.trimmed_sequences}")
					lfile.write("DONE\n")
			except Exception as e:
				with open(log['log'],'w') as lfile:
					lfile.write("ERROR: in trimal with exception: {}\n".format(e))
					lfile.write("WARNING: producing empty trimmed alignment file with touch\n")
					shell("touch {output.trimmed_sequences}")
#RULE 17
#mview creates an HTML file of the trimmed msa
rule display_alignment:
	input: trimmed_sequences="{qseqid}/msa_phylo_sequences_trimmed.msa"
	output: html_output="{qseqid}/msa_phylo_sequences_trimmed.html"
	log: display_alignment_log="log/{qseqid}/mview.log"
	shell:
		"mview -in fasta -html head -moltype aa -css on -coloring any -consensus on -sort cov {input.trimmed_sequences} > {output.html_output} 2> {log.display_alignment_log}"

#RULE 18
#phylogenetic inference with fasttree2
#this rule shouldn't throw exceptions, if an error occurs an empty file is created
rule conduct_phylogeny_fast_tree:
	input: target_msa="{qseqid}/msa_phylo_sequences_trimmed.msa"
	output: "{qseqid}/msa_phylo_sequences.tree"
	log: log="log/{qseqid}/phylogeny.log"
	run:
		#check if multiple sequence alignment has entries
		execute_fasttree=True
		with open(input[0],"r") as infile:
			if len(infile.readlines()) <= 2:
				#if there are no entries in the multiple sequence alignment there is no need for
				# conducting fasttree
				execute_fasttree == False
		try:
			#fasttree execution
			if execute_fasttree == True:
				fasttree = shell("fasttree {input.target_msa} > {output} 2> {log}")
				with open(log['log'],'a') as lfile:
					lfile.write("INFO:ending fasttree phylogeny construction\n")
					lfile.write("DONE\n")
			else:
				with open(log['log'],'w') as lfile:
					lfile.write("WARNING: producing empty tree file with touch\n")
					lfile.write("DONE\n")
				#produce empty file for snakemake
				shell("touch {output}")
		except Exception as e:
			with open(log['log'],'w') as lfile:
				lfile.write("ERROR: in fasttree with exception: {}\n".format(e))
				shell("touch {output}")

#RULE 19
#shiptv produces an interactive HTML file of the phylogeny
#an empty HTML document is produced if there are not > 2 sequences in the blast_table input
rule html_phylogenie:
    input: newick_tree="{qseqid}/msa_phylo_sequences.tree", blast_table="{qseqid}/msa_phylo_rbh_table.tsf",
    output: html_tree="{qseqid}/msa_phylo_sequences.html",
    log: shiptv_log="log/{qseqid}/shiptv.log",
    run:
        df = pd.read_table(input['blast_table'])
        if len(df) > 2:
            shell("shiptv --newick {input.newick_tree} --metadata {input.blast_table} --output-html {output.html_tree} 2> {log.shiptv_log}")
        else:
            with open(output['html_tree'], 'w') as html_output:
                html_output.write("<p>No sequences to display</p>\n")
            with open(log['shiptv_log'],'w') as logfile:
                logfile.write("WARNING:There are no sequences in the tree file\n")
                logfile.write("DONE")
#RULE 20
#rpsblast for query sequences with csv output for HTML documents
rule cdd_domain_rpsblast:
	input: queries=config['query_sequence'], domain_blast_log=expand("log/{qseqid}/shiptv.log", qseqid=QSEQIDS)
	params: e_value=config['fw_e_value'], num_threads=config['fw_num_threads']
	output: domain_df="query_domains.tsf"
	log: log="log/cdd_domain_rpsblast.log"
	shell:
		"rpsblast -query {input.queries} -db ../databases/CDD/Cdd -outfmt \"6 qseqid qlen sacc slen qstart qend sstart send qseq sseq bitscore evalue pident stitle\" -out {output.domain_df} -evalue {params.e_value} -num_threads {params.num_threads} 2> {log.log}"


#RULE 21
#rpsblast for query sequences with asn output for rpsbproc
rule cdd_domain_rpsblast_for_rpsbproc:
	input: queries=config['query_sequence'], domain_blast_log=expand("log/{qseqid}/shiptv.log", qseqid=QSEQIDS)
	params: e_value=config['fw_e_value'], num_threads=config['fw_num_threads']
	output: domain_df="query_domains.asn"
	log: log="log/cdd_domain_rpsblast_for_rpsbproc.log"
	shell:
		"rpsblast -query {input.queries} -db ../databases/CDD/Cdd -outfmt 11 -out {output.domain_df} -evalue {params.e_value} -num_threads {params.num_threads} 2> {log.log}"

#RULE 22
#rpsblast for query sequences with asn output for rpsbproc
rule cdd_domain_rpsbproc:
	input: rpsblast_asn_output="query_domains.asn", domain_blast_log="log/cdd_domain_rpsblast_for_rpsbproc.log"
	params: e_value=config['fw_e_value'], num_threads=config['fw_num_threads']
	output: rpsbproc_df="query_domains_rpsbproc.out"
	log: log="log/cdd_domain_rpsbproc.log"
	shell:
		"rpsbproc -i {input.rpsblast_asn_output} -o {output.rpsbproc_df} -e {params.e_value} -m rep -d /blast/utilities/RpsbProc-x64-linux/data/"

#RULE 23
#post-processing of the rpsbproc output, this rule produces interactive HTML documents
rule rpsbproc_parsing:
	input: rpsbproc_output="query_domains_rpsbproc.out"
	output: domain_html="rpsbproc_query_domains.html", sites_html="rpsbproc_query_sites.html", domain_csv="rpsbproc_query_domains.csv", sites_csv="rpsbproc_query_sites.csv"
	log: log="log/rpsbproc_parsing.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/rpsbproc_parser.py"

#RULE 24
#rpsblast output processing for possible CDD PCA execution within the conserved domain site of CATHI
rule query_domains_to_html_table:
	input: query_domains="query_domains.tsf", query_domains_html_table_log="log/cdd_domain_rpsblast.log"
	output: domain_html_table="query_domains.html"
	log: log="log/query_domains_to_html_table.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/query_sequence_domains_to_html_table.py"

#RULE 25
#this rule extracts the ftp paths to the proteomes of the RBHs for synteny analysis
rule extract_assembly_ftp_paths_from_rbhs:
	input: result_dataframe = "{qseqid}/rbh_table.tsf", query_domains_log="log/query_domains_to_html_table.log"
	output: ipg_table_path = "{qseqid}/ipg_table.table", genome_assembly_table = "{qseqid}/genome_assembly_table.csv"
	params: user_email=config['user_email'], assembly_file_path="../../../../media/databases/refseq_summary_file/"
	log: log="log/{qseqid}/extract_assembly_ftp_paths_from_rbhs.log"
	script:
		"../../../../static/snakefiles/reciprocal_blast/remote_searches/extract_assembly_ftp_paths_from_rbhs.py"

#RULE 26
#creates plots for the visualization of pipeline results
rule blast_tables_to_plots:
	input: fw_res="blastp_fw_out_reformatted.table", rec_res="reciprocal_best_hits_protein_ids.txt",query_file=config['query_sequence']
	output: taxids_hits_plot="plot_amount_hits_of_target_taxon.png",evalue_plot="plot_evalue_distribution.png"
	log: log="log/blast_tables_to_plots.log"
	script:
		  "../../../../static/snakefiles/reciprocal_blast/plot_reciprocal_results.py"